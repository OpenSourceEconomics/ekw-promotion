%!TEX root = ../main.tex
%-------------------------------------------------------------------------------
\section{Improvements}\label{Computation}
%-------------------------------------------------------------------------------
The implementation and analysis of this class of models entails several computational challenges. Among them integration of a high-dimensional non-differentiable function, large-scale global optimization of a noisy and non-smooth criterion function, function approximation, and parallelization strategies. We briefly outline each of them.
%-------------------------------------------------------------------------------
\subsection{Numerical integration} We want to draw on the extensive literature in applied math on numerical integration \citep{Davis.2007}. To clarify the structure of the integral determining the future value of a state, it is useful to consider the optimality equation in a generic time period $t$. This allows to focus on action-specific rewards instead of future values. Let $v^{\pi}_{t}(s_t, a_t)$ denote the action-specific value function of choosing action $a$ in state $s$ while continuing with the optimal policy going forward.
but sticking to the optimal policy $\pi^*$ going forward.
%
\begin{align}
v^{\pi}_{t}(s_t, a_t) & = u(s_t, a_t) + \delta\,\E_{s_t} \left[\left.v^{\pi^*}_{t + 1}(s_{t + 1})\,\right\vert\,\mathcal{I}_t\,\right] \\
& =  u(s_t, a_t) + \delta\, \int_S v^{\pi^*}_{t + 1}(s_{t + 1})\, \diff p_t(a_t, s_t)\\
& =  u(s_t, a_t) + \delta\, \underbrace{\int_S \max_{a \in A}\bigg\{v^\pi_{t + 1}(s_{t + 1}, a_{t + 1})\bigg\}\diff p_t(a_t, s_t)}_{\mathcal{I}(a_{t + 1})}.
\end{align}

\noindent The evaluation of such an integral is required millions of times during the backward induction procedure. The current practice is to implement a random Monte Carlo integration which introduces considerable numerical error and computational instabilities \citep{Judd.2011}.\\

\noindent Let's consider an atemporal version of the typical integral from \citet{Keane.1997}. In their model, individuals can choose among five alternatives. Each of the alternative-specific rewards is in part determined by a random continuous state variable that follows a normal distribution which happens to be unobserved. The transition of all observable state variables is deterministic. This results in a five-dimensional integrals the dimensionality is determined by the random state variables. The integral takes the following form:
%
\begin{align*}
   \int_{\epsilon}\, \max_{a\in A} \bigg\{v_{t + 1}^\pi(x_{t + 1}, \epsilon, a)\}\bigg\} \phi_{\mu, \Sigma}(\epsilon) \diff\epsilon.
\end{align*}

\noindent where $\epsilon = (\epsilon_1, \dots, \epsilon_5) \sim \mathcal{N}(\mu, \Sigma)\,$ follows a multivariate normal distribution with mean $\mu \in \mathbb{R}^{5}$, covariance matrix $ \Sigma \in \mathbb{R}^{5 \times 5}$, and probability density function $\phi_{\mu, \Sigma}$. A key features of this integral is the lack of general separability between the random and deterministic state variables. This makes the closed-form solutions impossible even under suitable distributional assumptions \citep{McFadden.1978,Rust.1987}.

\subsection{Global optimization} We want to draw on the specialized literature \citep{}.

\begin{itemize}
  \item \textbf{Likelihood-based estimation} This approach requires smoothing of the choice probabilities.

  \begin{align*}
    p_t(d_{it} \mid x_{it}, \theta) = \int \Ind [ \delta(x_{it}, \epsilon_{it}, \theta) = a_{it} ] g(\epsilon) \diff \epsilon
  \end{align*}

  \item \textbf{Simulation-based estimation} This approach requires the optimization of a noisy function.


\end{itemize}
%-------------------------------------------------------------------------------
\subsection{Miscellaneous} Function approximation, prallelization
